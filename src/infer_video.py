import cv2
import numpy as np
import joblib
import mediapipe as mp
from collections import Counter
from src.extract_regions import extract_regions
from src.optical_flow import calc_flows_gray_sequence
from src.extract_hof_features import extract_hof_from_sequence
from src.window_buffer import SlidingWindowBuffer


def preprocess_roi(roi, target_size=(128, 128)):
    """Aplica CLAHE, convierte a gris y redimensiona al tamaÃ±o usado en entrenamiento."""
    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    gray_eq = clahe.apply(gray)
    resized = cv2.resize(gray_eq, target_size)
    return resized


def run_inference_video(video_path):
    print(f"ğŸ¬ Analizando video: {video_path}")
    print("ğŸ“¦ Cargando modelo desde: models/svm_model.joblib")

    # Cargar modelo
    model_bundle = joblib.load("models/svm_model.joblib")
    if hasattr(model_bundle, "predict"):
        clf = model_bundle
        scaler = None
    else:
        clf = model_bundle["model"]
        scaler = model_bundle.get("scaler", None)

    # Inicializar MediaPipe
    mp_face_mesh = mp.solutions.face_mesh
    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=True)

    # Abrir video
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print("âŒ No se pudo abrir el video.")
        return

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    print(f"ğŸ FPS: {fps:.2f} | Total de frames: {total_frames}")

    buffer = SlidingWindowBuffer(window_size=10, stride=2)
    preds = []
    processed = 0

    print("\nğŸš€ Procesando video (presiona Q para salir)...\n")

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        processed += 1

        # Mostrar el video sin texto de emociÃ³n
        cv2.imshow("Inferencia (sin resultados en vivo)", frame)

        # Procesamiento del frame (sin mostrar emociÃ³n)
        regions = extract_regions(frame, face_mesh)
        rostro = regions.get("rostro_completo")
        if rostro is None:
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
            continue

        roi_pre = preprocess_roi(rostro)
        windows = buffer.add_frame(roi_pre)
        if not windows:
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break
            continue

        hof_vec = extract_hof_from_sequence(windows[0])
        hof_vec = np.array(hof_vec).reshape(1, -1)
        if scaler is not None:
            hof_vec = scaler.transform(hof_vec)

        pred = clf.predict(hof_vec)[0]
        preds.append(pred)

        if processed % 100 == 0:
            progress = (processed / total_frames) * 100
            print(f"ğŸ§© Progreso: {progress:.1f}% ({processed}/{total_frames})")

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ“Š Mostrar estadÃ­sticas finales
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if not preds:
        print("âš ï¸ No se generaron predicciones.")
        return

    print("\nâœ… Procesamiento completado.")
    print(f"ğŸ–¼ï¸ Frames analizados: {len(preds)}")

    counter = Counter(preds)
    total_preds = sum(counter.values())
    dominant, count = counter.most_common(1)[0]

    print("\nğŸ“ˆ EstadÃ­sticas finales:")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    for emotion, c in counter.most_common():
        percent = (c / total_preds) * 100
        print(f"{emotion:<15} â†’ {c:>5} frames ({percent:>5.1f}%)")

    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ğŸ­ EmociÃ³n dominante: {dominant} ({(count / total_preds) * 100:.1f}%)")

    # ğŸ’¾ Guardar resultados
    import csv
    output_csv = "models/video_inference_summary.csv"
    with open(output_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["frame_index", "predicted_emotion"])
        for i, p in enumerate(preds):
            writer.writerow([i, p])

    print(f"\nğŸ’¾ Resultados guardados en: {output_csv}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Inferencia de microexpresiones en video (resumen final).")
    parser.add_argument("--path", type=str, required=True, help="Ruta del video a analizar")
    args = parser.parse_args()

    run_inference_video(args.path)
